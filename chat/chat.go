// Package chat provides bindings for the [chat] [endpoint].
// Given a chat conversation, the model will return a chat
// completion response.
//
// [chat]: https://platform.openai.com/docs/api-reference/chat
// [endpoint]: https://api.openai.com/v1/chat/completions
package chat

import (
	"errors"
	"net/http"

	"github.com/Kardbord/gopenai/common"
	"github.com/Kardbord/gopenai/moderations"
)

const Endpoint = common.BaseURL + "chat/completions"

type Role = string

const (
	SystemRole    Role = "system"
	UserRole      Role = "user"
	AssistantRole Role = "assistant"
)

type FunctionCall struct {
	Arguments string `json:"arguments"`
	Name      string `json:"name"`
}

type ToolCall struct {
	ID       string       `json:"id"`
	Type     string       `json:"type"`
	Function FunctionCall `json:"function"`
}

type Chat struct {
	Content   string     `json:"content"`
	ToolCalls []ToolCall `json:"tool_calls,omitempty"`
	Role      Role       `json:"role"`

	// Deprecated: Use ToolCalls instead
	FunctionCall []FunctionCall `json:"function_call,omitempty"`
}

type ResponseFormat struct {
	// Must be one of text or json_object.
	Type string `json:"type,omitempty"`
}

// Request structure for the chat API endpoint.
type Request struct {
	// The messages to generate chat completions for,
	// in the [chat format].
	//
	// [chat format]: https://platform.openai.com/docs/guides/chat
	Messages []Chat `json:"messages"`

	// ID of the model to use. You can use the List models API
	// to see all of your available models, or see our Model
	// overview for descriptions of them.
	Model string `json:"model"`

	// Number between -2.0 and 2.0. Positive values penalize new
	// tokens based on their existing frequency in the text so far,
	// decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty *float64 `json:"frequency_penalty,omitempty"`

	// Modify the likelihood of specified tokens appearing in the completion.
	// Accepts a json object that maps tokens (specified by their token ID in
	// the tokenizer) to an associated bias value from -100 to 100. Mathematically,
	// the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease
	// or increase likelihood of selection; values like -100 or 100 should result in a
	// ban or exclusive selection of the relevant token.
	LogitBias map[string]int64 `json:"logit_bias,omitempty"`

	// The maximum number of tokens to generate in the chat completion.
	// The total length of input tokens and generated tokens is limited
	// by the model's context length.
	MaxTokens *int64 `json:"max_tokens,omitempty"`

	// How many chat completion choices to generate for each input message.
	N *int64 `json:"n,omitempty"`

	// Number between -2.0 and 2.0. Positive values penalize new tokens
	// based on their existing frequency in the text so far, decreasing
	// the model's likelihood to repeat the same line verbatim.
	PresencePenalty *float64 `json:"presence_penalty,omitempty"`

	// An object specifying the format that the model must output.
	// Setting to "json_object" enables JSON mode, which guarantees
	// the message the model generates is valid JSON.
	//
	// When using JSON mode, you must also instruct the model to produce
	// JSON yourself via a system or user message. Without this, the model
	// may generate an unending stream of whitespace until the generation
	// reaches the token limit, resulting in a long-running and seemingly
	// "stuck" request. Also note that the message content may be partially
	// cut off if finish_reason="length", which indicates the generation
	// exceeded max_tokens or the conversation exceeded the max context length.
	ResponseFormat *ResponseFormat `json:"response_format,omitempty"`

	// This feature is in Beta. If specified, our system will make a best effort
	// to sample deterministically, such that repeated requests with the same
	// seed and parameters should return the same result. Determinism is not
	// guaranteed, and you should refer to the system_fingerprint response
	// parameter to monitor changes in the backend.
	Seed int64 `json:"seed,omitempty"`

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"`

	// If set, partial message deltas will be sent, like in ChatGPT. Tokens
	// will be sent as data-only server-sent events as they become available,
	// with the stream terminated by a data: [DONE] message. See the OpenAI
	// Cookbook for example code.
	// Stream bool `json:"stream,omitempty"` TODO: Add streaming support

	// What sampling temperature to use, between 0 and 2. Higher values
	// like 0.8 will make the output more random, while lower values like
	// 0.2 will make it more focused and deterministic. We generally
	// recommend altering this or top_p but not both.
	Temperature *float64 `json:"temperature,omitempty"`

	// An alternative to sampling with temperature, called nucleus sampling,
	// where the model considers the results of the tokens with top_p
	// probability mass. So 0.1 means only the tokens comprising the top 10%
	// probability mass are considered.
	// We generally recommend altering this or temperature but not both.
	TopP *float64 `json:"top_p,omitempty"`

	// TODO: Support tools
	// Tools []Tool `json:"tools,omitempty"`
	// TODO: Support ToolChoice
	// ToolChoice ToolChoice `json:"tool_choice,omitempty"`

	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User string `json:"user,omitempty"`
}

type Response struct {
	ID      string `json:"id,omitempty"`
	Choices []struct {
		Index        int64  `json:"index,omitempty"`
		Message      Chat   `json:"message,omitempty"`
		FinishReason string `json:"finish_reason,omitempty"`
	}
	Created           int64                 `json:"created,omitempty"`
	Model             string                `json:"model,omitempty"`
	SystemFingerprint string                `json:"system_fingerprint,omitempty"`
	Object            string                `json:"object,omitempty"`
	Usage             common.ResponseUsage  `json:"usage"`
	Error             *common.ResponseError `json:"error,omitempty"`
}

func MakeRequest(request *Request, organizationID *string) (*Response, error) {
	r, err := common.MakeRequest[Request, Response](request, Endpoint, http.MethodPost, organizationID)
	if err != nil {
		return nil, err
	}
	if r == nil {
		return nil, errors.New("nil response received")
	}
	if r.Error != nil {
		return r, r.Error
	}
	if len(r.Choices) == 0 {
		return r, errors.New("no choices in response")
	}
	return r, nil
}

func MakeModeratedRequest(request *Request, organizationID *string) (*Response, *moderations.Response, error) {
	input := make([]string, len(request.Messages))
	for i := range request.Messages {
		input[i] = request.Messages[i].Content
	}

	modr, err := moderations.MakeModeratedRequest(&moderations.Request{
		Input: input,
		Model: moderations.ModelLatest,
	}, organizationID)
	if err != nil {
		return nil, modr, err
	}

	r, err := MakeRequest(request, organizationID)
	return r, modr, err
}
