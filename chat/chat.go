// Package chat provides bindings for the [chat] [endpoint].
// Given a chat conversation, the model will return a chat
// completion response.
//
// [chat]: https://platform.openai.com/docs/api-reference/chat
// [endpoint]: https://api.openai.com/v1/chat/completions
package chat

import (
	"errors"
	"net/http"

	"github.com/TannerKvarfordt/gopenai/common"
	"github.com/TannerKvarfordt/gopenai/moderations"
)

const Endpoint = common.BaseURL + "chat/completions"

type Role = string

const (
	SystemRole    Role = "system"
	UserRole      Role = "user"
	AssistantRole Role = "assistant"
)

type Chat struct {
	Role    Role   `json:"role"`
	Content string `json:"content"`
}

// Request structure for the chat API endpoint.
type Request struct {
	// ID of the model to use. You can use the List models API
	// to see all of your available models, or see our Model
	// overview for descriptions of them.
	Model string `json:"model"`

	// The messages to generate chat completions for,
	// in the [chat format].
	//
	// [chat format]: https://platform.openai.com/docs/guides/chat
	Messages []Chat `json:"messages"`

	// What sampling temperature to use, between 0 and 2. Higher values
	// like 0.8 will make the output more random, while lower values like
	// 0.2 will make it more focused and deterministic. We generally
	// recommend altering this or top_p but not both.
	Temperature *float64 `json:"temperature,omitempty"`

	// An alternative to sampling with temperature, called nucleus sampling,
	// where the model considers the results of the tokens with top_p
	// probability mass. So 0.1 means only the tokens comprising the top 10%
	// probability mass are considered.
	// We generally recommend altering this or temperature but not both.
	TopP *float64 `json:"top_p,omitempty"`

	// How many chat completion choices to generate for each input message.
	N *int64 `json:"n,omitempty"`

	// If set, partial message deltas will be sent, like in ChatGPT. Tokens
	// will be sent as data-only server-sent events as they become available,
	// with the stream terminated by a data: [DONE] message. See the OpenAI
	// Cookbook for example code.
	// Stream bool `json:"stream,omitempty"` TODO: Add streaming support

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"`

	// The maximum number of tokens to generate in the chat completion.
	// The total length of input tokens and generated tokens is limited
	// by the model's context length.
	MaxTokens *int64 `json:"max_tokens,omitempty"`

	// Number between -2.0 and 2.0. Positive values penalize new tokens
	// based on their existing frequency in the text so far, decreasing
	// the model's likelihood to repeat the same line verbatim.
	PresencePenalty *float64 `json:"presence_penalty,omitempty"`

	// Modify the likelihood of specified tokens appearing in the completion.
	// Accepts a json object that maps tokens (specified by their token ID in
	// the tokenizer) to an associated bias value from -100 to 100. Mathematically,
	// the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease
	// or increase likelihood of selection; values like -100 or 100 should result in a
	// ban or exclusive selection of the relevant token.
	LogitBias map[string]int64 `json:"logit_bias,omitempty"`

	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User string `json:"user,omitempty"`
}

type Response struct {
	ID      string `json:"id,omitempty"`
	Object  string `json:"object,omitempty"`
	Created int64  `json:"created,omitempty"`
	Choices []struct {
		Index        int64  `json:"index,omitempty"`
		Message      Chat   `json:"message,omitempty"`
		FinishReason string `json:"finish_reason,omitempty"`
	}
	Usage common.ResponseUsage  `json:"usage"`
	Error *common.ResponseError `json:"error,omitempty"`
}

func MakeRequest(request *Request, organizationID *string) (*Response, error) {
	r, err := common.MakeRequest[Request, Response](request, Endpoint, http.MethodPost, organizationID)
	if err != nil {
		return nil, err
	}
	if r == nil {
		return nil, errors.New("nil response received")
	}
	if r.Error != nil {
		return r, r.Error
	}
	if len(r.Choices) == 0 {
		return r, errors.New("no choices in response")
	}
	return r, nil
}

func MakeModeratedRequest(request *Request, organizationID *string) (*Response, *moderations.Response, error) {
	input := make([]string, len(request.Messages))
	for i := range request.Messages {
		input[i] = request.Messages[i].Content
	}

	modr, err := moderations.MakeModeratedRequest(&moderations.Request{
		Input: input,
		Model: moderations.ModelLatest,
	}, organizationID)
	if err != nil {
		return nil, modr, err
	}

	r, err := MakeRequest(request, organizationID)
	return r, modr, err
}
